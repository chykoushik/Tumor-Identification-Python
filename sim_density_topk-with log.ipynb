{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huisyuan/anaconda3/envs/tf_env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"norm_data__non_log.txt\",sep='\\t')\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "\n",
    "def extract_label(list): \n",
    "    number = '[0-9]'\n",
    "    symbol = '_'\n",
    "    head = 'Sample'\n",
    "    list = [re.sub(number, '', i) for i in list] \n",
    "    list = [re.sub(symbol, '', i) for i in list] \n",
    "    list = [re.sub(head, '', i) for i in list] \n",
    "    return list\n",
    "\n",
    "labels = list(data)\n",
    "labels = extract_label(labels)\n",
    "labels = np.ravel(labels)\n",
    "\n",
    "data=data.T\n",
    "\n",
    "mapping = {'Non-LCa':-1,'LCa':1}\n",
    "data[\"targets\"] = labels\n",
    "data[\"targets\"] = data[\"targets\"].map(mapping)\n",
    "#data\n",
    "\n",
    "#X = data.iloc[:,0:1183].as_matrix()\n",
    "#y = data[\"targets\"].as_matrix()\n",
    "#print(X.shape)\n",
    "#print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huisyuan/anaconda3/envs/tf_env/lib/python3.5/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Log and Scale Features in [0,1]\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "\n",
    "X = data.iloc[:,0:1183]\n",
    "X = np.log(X)\n",
    "data_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "#scale targets too\n",
    "#data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns) \n",
    "\n",
    "data_scaled\n",
    "X = data_scaled.as_matrix()\n",
    "y = data[\"targets\"].as_matrix\n",
    "# type(X)\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data.iloc[:,0:1183].as_matrix()\n",
    "# y = data[\"targets\"].as_matrix()\n",
    "# #print(X.shape)\n",
    "# #print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf with KDE\n",
    "\n",
    "# from sklearn.neighbors import KernelDensity\n",
    "# kde = KernelDensity(kernel='gaussian', bandwidth=0.25).fit(X)\n",
    "# kde.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_samples returns the log of the probability density\n",
    "\n",
    "# print(X[11])\n",
    "# print(X[200])\n",
    "\n",
    "# print(kde.score_samples(X[11].reshape(1,-1)),\n",
    "#       kde.score_samples(X[180].reshape(1,-1)),\n",
    "#       kde.score_samples(X[18].reshape(1,-1)),\n",
    "#       kde.score_samples(X[10].reshape(1,-1)),\n",
    "#       kde.score_samples(X[100].reshape(1,-1)),\n",
    "#       kde.score_samples(X[1000].reshape(1,-1)),)\n",
    "\n",
    "# logprob=kde.score_samples(X)\n",
    "# print(logprob)\n",
    "# print(np.exp(logprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try one pair\n",
    "\n",
    "# i = 1000\n",
    "# j = 100\n",
    "\n",
    "# from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# xgrid = np.linspace(0, 1, 1000)\n",
    "# kde_i = KernelDensity(kernel='gaussian', bandwidth=0.4).fit(X[:,[i]].reshape(-1,1))\n",
    "# kde_j = KernelDensity(kernel='gaussian', bandwidth=0.4).fit(X[:,[j]].reshape(-1,1))\n",
    "# sample_prob_i = kde_i.score_samples(xgrid.reshape(-1,1))\n",
    "# sample_prob_j = kde_j.score_samples(xgrid.reshape(-1,1))\n",
    "\n",
    "# #diff = np.sum((np.exp(sample_prob_i)-np.exp(sample_prob_j))**2)\n",
    "# diff = np.sum(((sample_prob_i)-(sample_prob_j))**2)\n",
    "# print(diff)\n",
    "\n",
    "\n",
    "# print(np.exp(sample_prob_i))\n",
    "# print(np.exp(sample_prob_j))\n",
    "# print(sample_prob_i)\n",
    "# print(sample_prob_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[651, 585, 299, 489, 534, 545, 667, 564, 573, 462, 680, 698, 466, 444, 395, 672, 532, 363, 404, 507, 616, 578, 669, 122, 645, 655, 367, 438, 473, 656, 303, 624, 471, 645, 341, 583, 689, 104, 334, 660, 395, 596, 438, 193, 592, 648, 643, 28, 361, 676, 679, 682, 616, 299, 660, 664, 653, 136, 682, 404, 680, 488, 527, 684, 565, 682, 642, 683, 673, 550, 352, 481, 672, 672, 398, 388, 670, 627, 668, 113, 670, 151, 638, 682, 422, 650, 519, 688, 203, 494, 532, 687, 425, 623, 617, 388, 673, 672, 466, 510, 599, 545, 669, 685, 631, 503, 366, 572, 563, 668, 337, 660, 292, 613, 630, 644, 694, 471, 567, 682, 671, 578, 388, 677, 451, 635, 444, 406, 683, 618, 693, 347, 677, 662, 493, 588, 203, 633, 615, 681, 672, 624, 503, 420, 595, 651, 698, 620, 575, 614, 404, 684, 679, 683, 671, 386, 414, 40, 671, 407, 682, 690, 655, 97, 178, 466, 388, 402, 651, 664, 673, 667, 530, 513, 683, 620, 545, 617, 686, 279, 673, 269, 297, 494, 684, 459, 617, 680, 545, 666, 684, 176, 668, 685, 342, 426, 99, 262, 426, 682, 686, 488, 590, 598, 366, 648, 623, 673, 146, 409, 681, 448, 79, 466, 393, 376, 616, 628, 508, 577, 425, 679, 363, 662, 483, 663, 502, 487, 570, 686, 457, 680, 264, 692, 378, 660, 495, 689, 667, 649, 682, 669, 666, 424, 541, 344, 110, 534, 696, 684, 109, 197, 686, 664, 231, 473, 685, 619, 564, 688, 281, 616, 680, 670, 587, 478, 675, 383, 682, 455, 582, 685, 89, 392, 687, 382, 673, 654, 673, 593, 674, 686, 539, 678, 393, 673, 684, 676, 654, 474, 666, 670, 261, 654, 676, 511, 662, 404, 684, 63, 441, 425, 612, 58, 404, 679, 649, 470, 593, 618, 94, 477, 672, 388, 685, 64, 604, 332, 677, 662, 604, 599, 423, 580, 675, 295, 98, 505, 488, 510, 546, 604, 675, 297, 684, 65, 685, 461, 685, 661, 639, 623, 394, 673, 345, 635, 667, 98, 564, 677, 107, 633, 658, 505, 567, 504, 696, 545, 113, 137, 309, 153, 695, 683, 546, 445, 674, 696, 573, 581, 665, 315, 460, 664, 586, 477, 680, 661, 674, 195, 682, 496, 695, 668, 535, 443, 357, 326, 353, 673, 671, 665, 670, 171, 647, 113, 683, 650, 329, 418, 615, 689, 665, 679, 492, 334, 163, 379, 684, 497, 662, 634, 232, 567, 680, 670, 607, 674, 451, 692, 434, 371, 685, 680, 520, 423, 674, 529, 651, 4, 396, 547, 79, 557, 526, 684, 71, 656, 473, 70, 337, 337, 489, 628, 133, 404, 674, 239, 609, 686, 388, 396, 599, 645, 688, 541, 540, 397, 331, 686, 114, 679, 599, 502, 497, 655, 674, 683, 458, 683, 386, 682, 505, 674, 354, 626, 649, 460, 76, 628, 556, 680, 332, 678, 682, 578, 679, 294, 661, 680, 679, 685, 399, 695, 480, 680, 564, 684, 297, 35, 649, 689, 365, 681, 681, 674, 585, 612, 477, 38, 490, 660, 478, 684, 518, 548, 374, 512, 332, 651, 668, 689, 685, 674, 689, 476, 248, 626, 451, 635, 622, 650, 170, 589, 652, 536, 678, 663, 264, 178, 667, 624, 685, 346, 666, 530, 660, 518, 64, 526, 669, 657, 378, 631, 444, 582, 687, 308, 532, 497, 586, 669, 538, 661, 452, 473, 416, 678, 544, 687, 697, 511, 405, 679, 299, 161, 666, 507, 561, 666, 684, 684, 298, 683, 181, 668, 678, 351, 388, 670, 615, 536, 492, 669, 566, 576, 279, 65, 389, 497, 482, 467, 670, 344, 603, 387, 651, 557, 575, 671, 504, 649, 454, 353, 633, 71, 671, 671, 298, 673, 649, 687, 114, 661, 602, 95, 69, 526, 698, 515, 162, 544, 589, 663, 652, 683, 666, 686, 474, 669, 687, 466, 691, 670, 682, 515, 584, 451, 684, 182, 685, 433, 616, 520, 394, 401, 628, 685, 406, 684, 674, 649, 580, 686, 668, 533, 679, 594, 656, 668, 642, 475, 390, 56, 417, 399, 492, 417, 389, 502, 648, 681, 431, 668, 685, 651, 317, 594, 13, 489, 663, 233, 612, 102, 69, 687, 626, 406, 502, 687, 389, 308, 683, 671, 686, 494, 694, 683, 684, 284, 296, 259, 679, 208, 533, 388, 252, 685, 561, 374, 521, 655, 647, 203, 680, 606, 661, 682, 667, 614, 76, 669, 622, 691, 515, 454, 675, 666, 655, 141, 505, 649, 590, 107, 588, 319, 234, 666, 683, 594, 340, 376, 665, 425, 520, 457, 665, 503, 589, 478, 582, 340, 388, 234, 475, 247, 456, 510, 668, 671, 474, 425, 124, 603, 522, 623, 522, 671, 655, 438, 668, 67, 679, 477, 670, 585, 686, 565, 662, 691, 256, 615, 161, 509, 681, 656, 692, 663, 663, 361, 680, 297, 686, 368, 674, 296, 516, 699, 337, 347, 684, 16, 685, 673, 95, 463, 406, 673, 673, 682, 613, 666, 596, 533, 318, 567, 673, 611, 376, 64, 527, 684, 599, 651, 633, 357, 650, 679, 412, 680, 81, 496, 513, 385, 371, 375, 648, 486, 686, 678, 454, 676, 13, 342, 686, 659, 475, 669, 650, 502, 635, 26, 451, 623, 686, 681, 673, 102, 585, 28, 649, 551, 649, 673, 685, 686, 600, 68, 163, 548, 635, 204, 665, 441, 547, 671, 683, 664, 607, 522, 652, 436, 690, 455, 106, 349, 348, 687, 362, 503, 527, 71, 693, 645, 340, 88, 681, 522, 699, 604, 349, 671, 679, 362, 296, 683, 691, 661, 340, 669, 684, 375, 469, 473, 692, 632, 455, 426, 678, 342, 532, 256, 638, 36, 691, 408, 676, 142, 102, 662, 673, 610, 507, 158, 648, 389, 560, 680, 650, 688, 353, 268, 664, 674, 498, 636, 556, 467, 337, 623, 259, 694, 122, 692, 672, 478, 66, 635, 682, 680, 640, 683, 462, 456, 544, 121, 535, 163, 314, 676, 621, 69, 327, 682, 603, 593, 665, 141, 426, 107, 502, 301, 280, 678, 308, 582, 396, 298, 619, 652, 455, 126, 562, 361, 647, 660, 607, 406, 476, 585, 481, 70, 664, 353, 697, 671, 398, 582, 322, 415, 340, 680, 672, 655, 271, 504, 674, 396, 382, 685, 496, 662, 261, 421, 673, 673, 679, 665, 327, 673, 689, 567, 95, 485, 475, 676, 70, 95, 667, 674, 685, 694, 622, 409, 350, 650, 199, 686, 681, 535, 674, 298, 677, 684, 539, 622, 670, 319, 339, 689, 437, 693, 13, 624, 164, 642, 320, 315, 679, 587, 335, 97, 676, 676, 686, 649, 122, 641, 695, 439, 424, 690, 668, 489, 693, 685, 674, 361, 684, 675, 477, 612, 624, 380, 9, 670, 659, 424, 656, 687, 357, 681, 203, 548, 582, 473, 248, 686, 478, 399, 573, 651, 481, 673, 612, 611, 576, 438, 564, 509, 687, 327, 388, 388, 555, 105, 103, 545, 160, 70, 472, 623, 693, 697, 648, 670, 535, 539, 508, 176, 646, 682, 692, 428, 486, 462, 527, 66, 604, 690, 690, 694, 682, 635, 687, 676, 404, 475, 663, 481, 688, 49, 677, 473, 648, 518, 670, 666, 607, 89, 439, 585, 582]\n"
     ]
    }
   ],
   "source": [
    "# All pairs\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "xgrid = np.linspace(0, 1, 10)\n",
    "#sim_table = pd.DataFrame()\n",
    "num_SimilarFeature = []\n",
    "\n",
    "for i in range(0,1183):\n",
    "    diff_i = []\n",
    "    kde_i = KernelDensity(kernel='gaussian', bandwidth=0.4).fit(X[:,[i]].reshape(-1,1))\n",
    "    sample_prob_i = kde_i.score_samples(xgrid.reshape(-1,1))\n",
    "    \n",
    "    for j in range(0,1183):\n",
    "        #if j!=i: \n",
    "\n",
    "        kde_j = KernelDensity(kernel='gaussian', bandwidth=0.4).fit(X[:,[j]].reshape(-1,1))\n",
    "        sample_prob_j = kde_j.score_samples(xgrid.reshape(-1,1))\n",
    "        diff = np.sum(((sample_prob_i)-(sample_prob_j))**2)   \n",
    "#         print(i,j,diff)\n",
    "        diff_i.append(diff)\n",
    "#     print(diff_i)\n",
    "#     the SimilarFeature includes itself\n",
    "    \n",
    "    num_SimilarFeature_i = np.sum(1 for s in diff_i if s<=1)\n",
    "    num_SimilarFeature.append(num_SimilarFeature_i)\n",
    "print(num_SimilarFeature)\n",
    "#     print(num_SimilarFeature_i)    \n",
    "#     sim_table[i] = diff_i\n",
    "# sim_table         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sim_density_logm.txt', 'w') as f:\n",
    "    for item in num_SimilarFeature:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best 33 :\n",
      " [47, 157, 299, 303, 315, 335, 429, 439, 499, 509, 548, 597, 626, 673, 688, 694, 781, 811, 829, 852, 861, 869, 877, 933, 966, 981, 1011, 1046, 1072, 1104, 1139, 1157, 1171]\n",
      "best 40 :\n",
      " [47, 157, 212, 299, 303, 315, 335, 429, 432, 436, 439, 478, 499, 509, 548, 597, 615, 626, 673, 688, 694, 730, 781, 811, 829, 852, 861, 869, 877, 901, 933, 966, 981, 1011, 1046, 1072, 1104, 1139, 1157, 1171]\n",
      "best 44 :\n",
      " [47, 157, 212, 272, 299, 303, 315, 335, 429, 432, 436, 439, 478, 499, 509, 548, 597, 615, 626, 673, 688, 694, 730, 781, 811, 829, 840, 852, 861, 869, 877, 901, 905, 933, 966, 981, 1011, 1046, 1072, 1104, 1139, 1157, 1171, 1179]\n"
     ]
    }
   ],
   "source": [
    "# Extract top-k distinct features \n",
    "\n",
    "k1 = [i for i in range(len(num_SimilarFeature)) if num_SimilarFeature[i] < 71]\n",
    "print('best',len(k1),':\\n',k1)\n",
    "\n",
    "k2 = [i for i in range(len(num_SimilarFeature)) if num_SimilarFeature[i] < 80]\n",
    "print('best',len(k2),':\\n',k2)\n",
    "\n",
    "k3 = [i for i in range(len(num_SimilarFeature)) if num_SimilarFeature[i] < 90]\n",
    "print('best',len(k3),':\\n',k3)\n",
    "\n",
    "with open('sim_density_log_top33.txt', 'w') as f:\n",
    "    for item in k1:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "        \n",
    "with open('sim_density_log_top40.txt', 'w') as f:\n",
    "    for item in k2:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "f.close()\n",
    "        \n",
    "with open('sim_density_log_top44.txt', 'w') as f:\n",
    "    for item in k3:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce Dimension\n",
    "\n",
    "# k2 = []\n",
    "# with open(\"sim_density_top30.txt\") as f:\n",
    "#     for line in f:\n",
    "#         k2.append(line)\n",
    "        \n",
    "# k2 = [float(i) for i in k2]        \n",
    "        \n",
    "# X_new = data.iloc[:,k2]\n",
    "# print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
